{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 20\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcopy\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01moptim\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nn, Tensor\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import math \n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout, Activation\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "from tensorflow.keras import optimizers\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import KNNImputer\n",
    "from numpy import linalg as LA\n",
    "import math\n",
    "import re\n",
    "import math\n",
    "import os\n",
    "from tempfile import TemporaryDirectory\n",
    "from typing import Tuple\n",
    "import copy\n",
    "import time\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch.nn import MultiheadAttention\n",
    "from torch.utils.data import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_rows = 1000000\n",
    "labe=pd.read_csv('labevents.csv',nrows=N_rows)\n",
    "labeinteret=labe.copy()\n",
    "labeinteret=labeinteret[['subject_id','specimen_id','itemid','charttime','storetime','valuenum','ref_range_lower','ref_range_upper','flag','priority']]\n",
    "labeinteret['intcharttime']=[int(re.sub(r\"\\D\", \"\", x)) if isinstance(x,str) else x for x in labeinteret['charttime']]\n",
    "labeinteret['intstoretime']=[int(re.sub(r\"\\D\", \"\", x)) if isinstance(x,str) else x for x in labeinteret['storetime']]\n",
    "labeinteret=labeinteret.drop(columns=['charttime','storetime'],axis=1)\n",
    "labeinteret=labeinteret.drop(columns=['flag','priority'],axis=1)\n",
    "labeinteret=labeinteret.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 783208 entries, 0 to 999994\n",
      "Data columns (total 8 columns):\n",
      " #   Column           Non-Null Count   Dtype  \n",
      "---  ------           --------------   -----  \n",
      " 0   subject_id       783208 non-null  int64  \n",
      " 1   specimen_id      783208 non-null  int64  \n",
      " 2   itemid           783208 non-null  int64  \n",
      " 3   valuenum         783208 non-null  float64\n",
      " 4   ref_range_lower  783208 non-null  float64\n",
      " 5   ref_range_upper  783208 non-null  float64\n",
      " 6   intcharttime     783208 non-null  int64  \n",
      " 7   intstoretime     783208 non-null  float64\n",
      "dtypes: float64(4), int64(4)\n",
      "memory usage: 53.8 MB\n"
     ]
    }
   ],
   "source": [
    "labeinteret.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j0/n1zmwvf923q5h9v_4n2k33100000gn/T/ipykernel_10407/3026321972.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  labeinteret['yrmeasure'][k]=int(labeinteretdatestr[k][:4])\n",
      "/var/folders/j0/n1zmwvf923q5h9v_4n2k33100000gn/T/ipykernel_10407/3026321972.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  labeinteret['monthmeasure'][k]=int(labeinteretdatestr[k][4:6])\n",
      "/var/folders/j0/n1zmwvf923q5h9v_4n2k33100000gn/T/ipykernel_10407/3026321972.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  labeinteret['daymeasure'][k]=int(labeinteretdatestr[k][6:8])\n",
      "/var/folders/j0/n1zmwvf923q5h9v_4n2k33100000gn/T/ipykernel_10407/3026321972.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  labeinteret['hrmeasure'][k]=int(labeinteretdatestr[k][8:10])\n",
      "/var/folders/j0/n1zmwvf923q5h9v_4n2k33100000gn/T/ipykernel_10407/3026321972.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  labeinteret['minmeasure'][k]=int(labeinteretdatestr[k][10:12])\n",
      "/var/folders/j0/n1zmwvf923q5h9v_4n2k33100000gn/T/ipykernel_10407/3026321972.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  labeinteret['secmeasure'][k]=int(labeinteretdatestr[k][12:14])\n"
     ]
    }
   ],
   "source": [
    "Nrest=783208\n",
    "labeinteretdatestr= labeinteret['intcharttime'].astype(str)\n",
    "labeinteret['yrmeasure']=np.zeros(Nrest)\n",
    "labeinteret['monthmeasure']=np.zeros(Nrest)\n",
    "labeinteret['daymeasure']=np.zeros(Nrest)\n",
    "labeinteret['hrmeasure']=np.zeros(Nrest)\n",
    "labeinteret['minmeasure']=np.zeros(Nrest)\n",
    "labeinteret['minmeasure']=np.zeros(Nrest)\n",
    "labeinteret['secmeasure']=np.zeros(Nrest)\n",
    "\n",
    "for k in range(783208):\n",
    "    if k in labeinteretdatestr :\n",
    "        labeinteret['yrmeasure'][k]=int(labeinteretdatestr[k][:4])\n",
    "        labeinteret['monthmeasure'][k]=int(labeinteretdatestr[k][4:6])\n",
    "        labeinteret['daymeasure'][k]=int(labeinteretdatestr[k][6:8])\n",
    "        labeinteret['hrmeasure'][k]=int(labeinteretdatestr[k][8:10])\n",
    "        labeinteret['minmeasure'][k]=int(labeinteretdatestr[k][10:12])\n",
    "        labeinteret['secmeasure'][k]=int(labeinteretdatestr[k][12:14])\n",
    "        \n",
    "labeinteret['yrmeasure']=labeinteret['yrmeasure'].astype(int)\n",
    "labeinteret= labeinteret.sort_values(['subject_id','yrmeasure','monthmeasure','daymeasure','hrmeasure','minmeasure'])\n",
    "labeinteret = labeinteret.drop('intstoretime',axis=1)\n",
    "labeinteret = labeinteret.drop('secmeasure',axis=1)\n",
    "labeinteret = labeinteret.drop('minmeasure',axis=1)\n",
    "labeinteret = labeinteret.drop('hrmeasure',axis=1)\n",
    "labeinteret = labeinteret.drop('daymeasure',axis=1)\n",
    "#labeinteret = labeinteret.drop(['ref_range_lower','ref_range_upper','yrmeasure','monthmeasure'],axis=1)\n",
    "labeinteretarray= labeinteret.to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "listindiv = list(set(labeinteretarray[:,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "sepers =[]\n",
    "for i in range(len(listindiv)) :\n",
    "    sepers.append([])\n",
    "for x in labeinteretarray :\n",
    "    sepers[listindiv.index(x[0])].append(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1212"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(x) for x in sepers].index(max([len(x) for x in sepers]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "meclong=np.array(sepers[1212])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "listspec=list(set(meclong[:,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "sepersanal =[]\n",
    "for i in range(len(listspec)) :\n",
    "    sepersanal.append([])\n",
    "for x in meclong :\n",
    "    sepersanal[listspec.index(x[2])].append(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(x) for x in sepersanal].index(max([len(x) for x in sepersanal]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 78528 entries, 0 to 99999\n",
      "Data columns (total 9 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   subject_id       78528 non-null  int64  \n",
      " 1   specimen_id      78528 non-null  int64  \n",
      " 2   itemid           78528 non-null  int64  \n",
      " 3   valuenum         78528 non-null  float64\n",
      " 4   ref_range_lower  78528 non-null  float64\n",
      " 5   ref_range_upper  78528 non-null  float64\n",
      " 6   intcharttime     78528 non-null  int64  \n",
      " 7   yrmeasure        78528 non-null  int64  \n",
      " 8   monthmeasure     78528 non-null  float64\n",
      "dtypes: float64(4), int64(5)\n",
      "memory usage: 6.0 MB\n"
     ]
    }
   ],
   "source": [
    "labeinteret.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14072"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sepers[1212])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "h=list(set(labeinteretarray[:,0]))\n",
    "tableau=[]\n",
    "for k in range(len(h)) :\n",
    "    tableau.append([])\n",
    "for x in labeinteretarray :\n",
    "    tableau[h.index(x[0])].append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        \n",
    "        # Compute the positional encodings once in log space\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Add the positional encodings in the time dimension\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define the transformer model\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, nhead, num_layers):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.pos = PositionalEncoding(d_model=4)\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(input_dim, nhead), num_layers\n",
    "        )\n",
    "        self.linear = nn.Linear(input_dim * 24, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pos(x)\n",
    "        # x has shape (seq_len, batch, input_dim)\n",
    "        x = self.transformer(x)\n",
    "        # x has shape (seq_len, batch, input_dim)\n",
    "        x = x.transpose(0, 1)\n",
    "        # x has shape (batch, seq_len, input_dim)\n",
    "        x = x.flatten(start_dim=1)\n",
    "        # x has shape (batch, seq_len * input_dim)\n",
    "        x = self.linear(x)\n",
    "        # x has shape (batch, output_dim)\n",
    "        return x\n",
    "\n",
    "# Define the training loop\n",
    "def train(model, optimizer, criterion, X, y, batch_size, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for i in range(0, len(X) - batch_size, batch_size):\n",
    "            optimizer.zero_grad()\n",
    "            inputs = torch.tensor(X[i:i+batch_size]).float().transpose(0, 1)\n",
    "            targets = torch.tensor(y[i:i+batch_size]).float()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item() * batch_size\n",
    "        print(\"Epoch {}, Loss: {}\".format(epoch+1, total_loss / len(X)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate some random data\n",
    "data = np.array(sepersanal[18])\n",
    "data = data[:,5:]\n",
    "data_mean = np.mean(data,axis=0)\n",
    "data_std = np.std(data,axis=0)\n",
    "data = (data - data_mean) / data_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.98311097, -1.419208  , -1.39724612,  0.38909599],\n",
       "       [ 0.98311097, -1.41908893, -1.39724612,  0.38909599],\n",
       "       [ 0.98311097, -1.41908093, -1.39724612,  0.38909599],\n",
       "       ...,\n",
       "       [-1.01717917,  1.31648074,  1.28624993, -0.17202139],\n",
       "       [-1.01717917,  1.31648658,  1.28624993, -0.17202139],\n",
       "       [-1.01717917,  1.31660739,  1.28624993, -0.17202139]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.1650974904835032\n",
      "Epoch 2, Loss: 2.1230349957364276\n",
      "Epoch 3, Loss: 1.55009297954226\n",
      "Epoch 4, Loss: 0.9900228722581585\n",
      "Epoch 5, Loss: 1.0037159765422536\n",
      "Epoch 6, Loss: 0.9458726463194418\n",
      "Epoch 7, Loss: 0.8372472968302113\n",
      "Epoch 8, Loss: 0.6013258930934672\n",
      "Epoch 9, Loss: 0.763569129709287\n",
      "Epoch 10, Loss: 1.1263110822844273\n",
      "Epoch 11, Loss: 0.9359262413963145\n",
      "Epoch 12, Loss: 0.700408707158851\n",
      "Epoch 13, Loss: 0.6784135176525918\n",
      "Epoch 14, Loss: 0.6016466208646213\n",
      "Epoch 15, Loss: 0.5857152784526541\n",
      "Epoch 16, Loss: 0.5329817730246238\n",
      "Epoch 17, Loss: 0.5318879340458842\n",
      "Epoch 18, Loss: 0.47384554359905157\n",
      "Epoch 19, Loss: 0.3578713534333559\n",
      "Epoch 20, Loss: 0.489583560178195\n",
      "Epoch 21, Loss: 0.634960993979741\n",
      "Epoch 22, Loss: 0.5117061300185121\n",
      "Epoch 23, Loss: 0.5988904974607202\n",
      "Epoch 24, Loss: 0.3800906630588581\n",
      "Epoch 25, Loss: 0.24753141866146938\n",
      "Epoch 26, Loss: 0.2783279970625843\n",
      "Epoch 27, Loss: 0.22516813710283692\n",
      "Epoch 28, Loss: 0.27180234822640525\n",
      "Epoch 29, Loss: 0.1952942802682278\n",
      "Epoch 30, Loss: 0.42349707347289645\n",
      "Epoch 31, Loss: 0.33465590368968384\n",
      "Epoch 32, Loss: 0.43419353707322794\n",
      "Epoch 33, Loss: 0.1774942510336348\n",
      "Epoch 34, Loss: 0.20050270657709116\n",
      "Epoch 35, Loss: 0.33022515789204815\n",
      "Epoch 36, Loss: 0.3135057878725737\n",
      "Epoch 37, Loss: 0.36224866569235104\n",
      "Epoch 38, Loss: 0.27641411352311906\n",
      "Epoch 39, Loss: 0.3189183836230182\n",
      "Epoch 40, Loss: 0.20419630167167935\n",
      "Epoch 41, Loss: 0.2509953341051984\n",
      "Epoch 42, Loss: 0.22938294568879705\n",
      "Epoch 43, Loss: 0.3551906501591013\n",
      "Epoch 44, Loss: 0.24439803064833954\n",
      "Epoch 45, Loss: 0.2439271157613464\n",
      "Epoch 46, Loss: 0.20784153444481512\n",
      "Epoch 47, Loss: 0.21487327571054107\n",
      "Epoch 48, Loss: 0.1934662496001975\n",
      "Epoch 49, Loss: 0.24454365868398673\n",
      "Epoch 50, Loss: 0.3165621819233817\n",
      "Validation Loss: 0.1136423870921135\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Split the data into input and output sequences\n",
    "X, y = [], []\n",
    "for i in range(24, len(data)):\n",
    "    X.append(data[i-24:i])\n",
    "    y.append(data[i])\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "    # Split the data into training and validation sets\n",
    "split = int(0.8 * len(X))\n",
    "X_train, y_train = X[:split], y[:split]\n",
    "X_val, y_val = X[split:], y[split:]\n",
    "    # Define the model, optimizer, and loss function\n",
    "model = TransformerModel(4, 4, nhead=2, num_layers=2)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = nn.MSELoss()\n",
    "    # Train the model\n",
    "train(model, optimizer, criterion, X_train, y_train, batch_size=16, num_epochs=50)\n",
    "    # Evaluate the model on the validation set\n",
    "inputs = torch.tensor(X_val).float().transpose(0, 1)\n",
    "targets = torch.tensor(y_val).float()\n",
    "outputs = model(inputs)\n",
    "loss = criterion(outputs, targets)\n",
    "print(\"Validation Loss:\", loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.6299,  0.9090,  0.9480, -0.6599], grad_fn=<SelectBackward0>)\n",
      "[-1.01717917  1.286105    1.28624993 -0.73313877]\n",
      "tensor([-0.1364,  0.5756,  0.5728, -0.6740], grad_fn=<SelectBackward0>)\n",
      "[-1.01717917  1.28623583  1.28624993 -0.73313877]\n",
      "tensor([ 0.0449,  0.4852,  0.5663, -0.5580], grad_fn=<SelectBackward0>)\n",
      "[-1.01717917  1.28637391  1.28624993 -0.73313877]\n",
      "tensor([-0.8305,  1.0850,  1.0687, -0.8772], grad_fn=<SelectBackward0>)\n",
      "[-1.01717917  1.28651286  1.28624993 -0.73313877]\n",
      "tensor([-0.6149,  0.7549,  0.7575, -0.3637], grad_fn=<SelectBackward0>)\n",
      "[-1.01717917  1.28652805  1.28624993 -0.73313877]\n",
      "tensor([-0.7989,  0.8809,  0.8572, -0.4208], grad_fn=<SelectBackward0>)\n",
      "[-1.01717917  1.28665105  1.28624993 -0.73313877]\n",
      "tensor([-0.8858,  0.9358,  0.9144, -0.5299], grad_fn=<SelectBackward0>)\n",
      "[-1.01717917  1.28707665  1.28624993 -0.73313877]\n",
      "tensor([-0.2462,  0.7912,  0.8170, -0.7766], grad_fn=<SelectBackward0>)\n",
      "[-1.01717917  1.28762691  1.28624993 -0.73313877]\n",
      "tensor([-0.7216,  0.9444,  0.9565, -0.5608], grad_fn=<SelectBackward0>)\n",
      "[-1.01717917  1.28763422  1.28624993 -0.73313877]\n",
      "tensor([-0.6158,  0.9636,  0.9254, -0.6183], grad_fn=<SelectBackward0>)\n",
      "[-1.01717917  1.28776558  1.28624993 -0.73313877]\n",
      "tensor([-0.5137,  0.5886,  0.5666, -0.4153], grad_fn=<SelectBackward0>)\n",
      "[-1.01717917  1.28803891  1.28624993 -0.73313877]\n",
      "tensor([-0.7610,  0.8983,  0.9236, -0.6056], grad_fn=<SelectBackward0>)\n",
      "[-1.01717917  1.28859106  1.28624993 -0.73313877]\n",
      "tensor([-0.2467,  0.9503,  0.9114, -0.6124], grad_fn=<SelectBackward0>)\n",
      "[-1.01717917  1.28859494  1.28624993 -0.73313877]\n",
      "tensor([-0.6279,  0.9057,  0.9449, -0.7928], grad_fn=<SelectBackward0>)\n",
      "[-1.01717917  1.28900387  1.28624993 -0.73313877]\n",
      "tensor([-0.4675,  0.8731,  0.8712, -0.7444], grad_fn=<SelectBackward0>)\n",
      "[-1.01717917  1.28900897  1.28624993 -0.73313877]\n",
      "tensor([-0.7184,  1.0193,  1.0171, -0.6650], grad_fn=<SelectBackward0>)\n",
      "[-1.01717917  1.29921886  1.28624993 -0.45258008]\n",
      "tensor([-0.5852,  0.9658,  0.9476, -0.8411], grad_fn=<SelectBackward0>)\n",
      "[-1.01717917  1.29949669  1.28624993 -0.45258008]\n",
      "tensor([-0.9271,  1.0152,  1.0452, -0.3662], grad_fn=<SelectBackward0>)\n",
      "[-1.01717917  1.29964177  1.28624993 -0.45258008]\n",
      "tensor([-0.7550,  0.8881,  0.8683, -0.3225], grad_fn=<SelectBackward0>)\n",
      "[-1.01717917  1.29964398  1.28624993 -0.45258008]\n",
      "tensor([-0.7509,  1.0357,  1.0015, -0.5812], grad_fn=<SelectBackward0>)\n",
      "[-1.01717917  1.30004632  1.28624993 -0.45258008]\n",
      "tensor([-0.9923,  0.9003,  0.8967, -0.3838], grad_fn=<SelectBackward0>)\n",
      "[-1.01717917  1.30046316  1.28624993 -0.45258008]\n",
      "tensor([-0.6031,  1.1086,  1.1052, -0.7598], grad_fn=<SelectBackward0>)\n",
      "[-1.01717917  1.3004682   1.28624993 -0.45258008]\n",
      "tensor([-0.9255,  1.0672,  1.0703, -0.6191], grad_fn=<SelectBackward0>)\n",
      "[-1.01717917  1.30073788  1.28624993 -0.45258008]\n",
      "tensor([-1.2809,  0.9423,  0.9509, -0.0734], grad_fn=<SelectBackward0>)\n",
      "[-1.01717917  1.30074333  1.28624993 -0.45258008]\n",
      "tensor([-0.9120,  1.0993,  1.0823, -0.6356], grad_fn=<SelectBackward0>)\n",
      "[-1.01717917  1.30115083  1.28624993 -0.45258008]\n",
      "tensor([-0.7212,  0.8968,  0.9203, -0.3825], grad_fn=<SelectBackward0>)\n",
      "[-1.01717917  1.30115851  1.28624993 -0.45258008]\n",
      "tensor([-0.9645,  1.0117,  0.9667, -0.4154], grad_fn=<SelectBackward0>)\n",
      "[-1.01717917  1.30142674  1.28624993 -0.45258008]\n",
      "tensor([-0.7787,  1.1420,  1.1372, -0.8503], grad_fn=<SelectBackward0>)\n",
      "[-1.01717917  1.30143171  1.28624993 -0.45258008]\n",
      "tensor([-1.2529,  1.1391,  1.1212, -0.2966], grad_fn=<SelectBackward0>)\n",
      "[-1.01717917  1.3017041   1.28624993 -0.45258008]\n",
      "tensor([-1.0807,  1.0768,  1.1284, -0.4538], grad_fn=<SelectBackward0>)\n",
      "[-1.01717917  1.30170714  1.28624993 -0.45258008]\n",
      "tensor([-0.9314,  1.2152,  1.1805, -0.7151], grad_fn=<SelectBackward0>)\n",
      "[-1.01717917  1.30197976  1.28624993 -0.45258008]\n",
      "tensor([-0.6442,  0.9089,  0.9542, -0.5571], grad_fn=<SelectBackward0>)\n",
      "[-1.01717917  1.30198445  1.28624993 -0.45258008]\n",
      "tensor([-0.8991,  0.9796,  0.9641, -0.5298], grad_fn=<SelectBackward0>)\n",
      "[-1.01717917  1.30239556  1.28624993 -0.45258008]\n",
      "tensor([-0.7719,  0.9616,  0.9375, -0.7078], grad_fn=<SelectBackward0>)\n",
      "[-1.01717917  1.30239977  1.28624993 -0.45258008]\n",
      "tensor([-0.9466,  0.9198,  0.9095, -0.2762], grad_fn=<SelectBackward0>)\n",
      "[-1.01717917  1.30266931  1.28624993 -0.45258008]\n",
      "tensor([-0.4418,  0.9578,  0.9207, -0.5709], grad_fn=<SelectBackward0>)\n",
      "[-1.01717917  1.30267296  1.28624993 -0.45258008]\n",
      "tensor([-0.7299,  1.0639,  1.0757, -0.6887], grad_fn=<SelectBackward0>)\n",
      "[-1.01717917  1.30308593  1.28624993 -0.45258008]\n",
      "tensor([-1.2747,  1.2479,  1.1763, -0.2866], grad_fn=<SelectBackward0>)\n",
      "[-1.01717917  1.30308994  1.28624993 -0.45258008]\n",
      "tensor([-0.7902,  1.1825,  1.1465, -0.8510], grad_fn=<SelectBackward0>)\n",
      "[-1.01717917  1.31288727  1.28624993 -0.17202139]\n",
      "tensor([-1.0167,  1.1798,  1.1900, -0.7269], grad_fn=<SelectBackward0>)\n",
      "[-1.01717917  1.31302815  1.28624993 -0.17202139]\n",
      "tensor([-0.8255,  0.7653,  0.7518, -0.0655], grad_fn=<SelectBackward0>)\n",
      "[-1.01717917  1.31303091  1.28624993 -0.17202139]\n",
      "tensor([-0.4207,  0.8127,  0.7746, -0.6023], grad_fn=<SelectBackward0>)\n",
      "[-1.01717917  1.31330038  1.28624993 -0.17202139]\n",
      "tensor([-0.7674,  0.9068,  0.9235, -0.5102], grad_fn=<SelectBackward0>)\n",
      "[-1.01717917  1.31343801  1.28624993 -0.17202139]\n",
      "tensor([-1.1422,  1.2624,  1.2295, -0.5285], grad_fn=<SelectBackward0>)\n",
      "[-1.01717917  1.31344229  1.28624993 -0.17202139]\n",
      "tensor([-1.0579,  1.0018,  1.0105, -0.5213], grad_fn=<SelectBackward0>)\n",
      "[-1.01717917  1.31357868  1.28624993 -0.17202139]\n",
      "tensor([-0.7515,  1.0130,  0.9865, -0.6213], grad_fn=<SelectBackward0>)\n",
      "[-1.01717917  1.31385216  1.28624993 -0.17202139]\n",
      "tensor([-1.1309,  0.9403,  0.9397, -0.1669], grad_fn=<SelectBackward0>)\n",
      "[-1.01717917  1.31385805  1.28624993 -0.17202139]\n",
      "tensor([-0.7380,  0.9036,  0.8935, -0.2572], grad_fn=<SelectBackward0>)\n",
      "[-1.01717917  1.31397792  1.28624993 -0.17202139]\n",
      "tensor([-0.9908,  0.9373,  0.8497, -0.1484], grad_fn=<SelectBackward0>)\n",
      "[-1.01717917  1.31398606  1.28624993 -0.17202139]\n",
      "tensor([-0.7182,  1.0685,  1.0875, -0.7918], grad_fn=<SelectBackward0>)\n",
      "[-1.01717917  1.31399276  1.28624993 -0.17202139]\n",
      "tensor([-0.9638,  0.9029,  0.8707, -0.1505], grad_fn=<SelectBackward0>)\n",
      "[-1.01717917  1.31400415  1.28624993 -0.17202139]\n",
      "tensor([-0.7740,  1.0422,  1.0071, -0.5899], grad_fn=<SelectBackward0>)\n",
      "[-1.01717917  1.31412439  1.28624993 -0.17202139]\n",
      "tensor([-1.0237,  1.0459,  1.1213, -0.5672], grad_fn=<SelectBackward0>)\n",
      "[-1.01717917  1.3142605   1.28624993 -0.17202139]\n",
      "tensor([-0.9054,  0.8841,  0.8482, -0.2852], grad_fn=<SelectBackward0>)\n",
      "[-1.01717917  1.31439862  1.28624993 -0.17202139]\n",
      "tensor([-0.4617,  1.0066,  0.9970, -0.7617], grad_fn=<SelectBackward0>)\n",
      "[-1.01717917  1.31453591  1.28624993 -0.17202139]\n",
      "tensor([-1.3712,  1.0721,  1.0862, -0.3079], grad_fn=<SelectBackward0>)\n",
      "[-1.01717917  1.31455334  1.28624993 -0.17202139]\n",
      "tensor([-1.0122,  1.1253,  1.1017, -0.4302], grad_fn=<SelectBackward0>)\n",
      "[-1.01717917  1.31467485  1.28624993 -0.17202139]\n",
      "tensor([-0.9470,  1.1798,  1.1989, -0.8104], grad_fn=<SelectBackward0>)\n",
      "[-1.01717917  1.31480455  1.28624993 -0.17202139]\n",
      "tensor([-0.9536,  1.0350,  1.0132, -0.3891], grad_fn=<SelectBackward0>)\n",
      "[-1.01717917  1.31494246  1.28624993 -0.17202139]\n",
      "tensor([-1.1101,  1.2426,  1.2102, -0.5058], grad_fn=<SelectBackward0>)\n",
      "[-1.01717917  1.31508051  1.28624993 -0.17202139]\n",
      "tensor([-0.9005,  0.7399,  0.7318, -0.1417], grad_fn=<SelectBackward0>)\n",
      "[-1.01717917  1.31522132  1.28624993 -0.17202139]\n",
      "tensor([-1.0199,  1.1716,  1.1634, -0.6169], grad_fn=<SelectBackward0>)\n",
      "[-1.01717917  1.31522828  1.28624993 -0.17202139]\n",
      "tensor([-0.7935,  1.0446,  1.0713, -0.7922], grad_fn=<SelectBackward0>)\n",
      "[-1.01717917  1.31535667  1.28624993 -0.17202139]\n",
      "tensor([-0.9210,  1.0547,  1.0604, -0.6815], grad_fn=<SelectBackward0>)\n",
      "[-1.01717917  1.31549465  1.28624993 -0.17202139]\n",
      "tensor([-0.7754,  0.8738,  0.8220, -0.4322], grad_fn=<SelectBackward0>)\n",
      "[-1.01717917  1.31552113  1.28624993 -0.17202139]\n",
      "tensor([-0.8239,  1.1525,  1.1542, -0.8418], grad_fn=<SelectBackward0>)\n",
      "[-1.01717917  1.31563273  1.28624993 -0.17202139]\n",
      "tensor([-0.9883,  0.9943,  1.0705, -0.5105], grad_fn=<SelectBackward0>)\n",
      "[-1.01717917  1.31577075  1.28624993 -0.17202139]\n",
      "tensor([-0.9580,  1.1925,  1.1286, -0.5088], grad_fn=<SelectBackward0>)\n",
      "[-1.01717917  1.3159088   1.28624993 -0.17202139]\n",
      "tensor([-0.8780,  1.0592,  1.0903, -0.5570], grad_fn=<SelectBackward0>)\n",
      "[-1.01717917  1.3159148   1.28624993 -0.17202139]\n",
      "tensor([-0.9785,  0.7971,  0.7804, -0.1266], grad_fn=<SelectBackward0>)\n",
      "[-1.01717917  1.3160544   1.28624993 -0.17202139]\n",
      "tensor([-0.9574,  0.9748,  0.9623, -0.2275], grad_fn=<SelectBackward0>)\n",
      "[-1.01717917  1.31605827  1.28624993 -0.17202139]\n",
      "tensor([-0.7093,  0.9824,  0.9475, -0.3706], grad_fn=<SelectBackward0>)\n",
      "[-1.01717917  1.31606913  1.28624993 -0.17202139]\n",
      "tensor([-0.5161,  1.0483,  1.0222, -0.8241], grad_fn=<SelectBackward0>)\n",
      "[-1.01717917  1.31619345  1.28624993 -0.17202139]\n",
      "tensor([-0.7265,  1.0975,  1.0890, -0.7656], grad_fn=<SelectBackward0>)\n",
      "[-1.01717917  1.31632922  1.28624993 -0.17202139]\n",
      "tensor([-0.6173,  0.9763,  0.9939, -0.6675], grad_fn=<SelectBackward0>)\n",
      "[-1.01717917  1.31647079  1.28624993 -0.17202139]\n",
      "tensor([-0.7004,  1.0240,  1.0371, -0.6397], grad_fn=<SelectBackward0>)\n",
      "[-1.01717917  1.31648074  1.28624993 -0.17202139]\n",
      "tensor([-0.7437,  0.8674,  0.8914, -0.5717], grad_fn=<SelectBackward0>)\n",
      "[-1.01717917  1.31648658  1.28624993 -0.17202139]\n",
      "tensor([-0.8552,  0.8127,  0.7655, -0.1627], grad_fn=<SelectBackward0>)\n",
      "[-1.01717917  1.31660739  1.28624993 -0.17202139]\n"
     ]
    }
   ],
   "source": [
    "sortie=outputs\n",
    "\n",
    "sortiexpct=y_val\n",
    "for k in range(len(sortie)) :\n",
    "    print(sortie[k])\n",
    "    print(sortiexpct[k])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
